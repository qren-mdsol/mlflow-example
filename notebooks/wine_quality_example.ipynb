{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a218b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bbbcb0",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import mlflow.sklearn\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils as u\n",
    "from visualization_helpers import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7c8be",
   "metadata": {
    "title": "CONFIGURATION"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(40)\n",
    "\n",
    "# In the UI run_names are grouped under experiment names\n",
    "# We will group the data as the experiment and modeling approach as the runs\n",
    "# Note without a run_name MLFlow will makeup a name and without an Experiment Name MLFlow will use \"Default\"\n",
    "\n",
    "experiment_name=\"Wine Quality\"\n",
    "run_name=\"elastic_net_intro\"\n",
    "\n",
    "experiment_id=u.get_or_create_experiment(experiment_name)\n",
    "\n",
    "alpha = .5\n",
    "l1_ratio = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca37d9",
   "metadata": {
    "title": "DATA PREP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the wine-quality csv file\n",
    "current_file_path=os.path.dirname(__file__)\n",
    "wine_path = os.path.join(current_file_path,\"../data\", \"wine-quality.csv\")\n",
    "data = pd.read_csv(wine_path)\n",
    "\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[\"quality\"]\n",
    "test_y = test[\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f4e6d",
   "metadata": {
    "lines_to_next_cell": 3,
    "title": "MLFLOW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# We don't want duplicate run names when the code is executed many times\n",
    "u.clear_run(experiment_id,run_name)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name,experiment_id=experiment_id):\n",
    "    \n",
    "    # tags are optional but can be helpful if you have many experiments and you later\n",
    "    # want to search and filter among them in the UI\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Wine Quality\",\n",
    "            \"model_family\": \"elastic_net\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    lr = ElasticNet(alpha = alpha,l1_ratio = l1_ratio,random_state=42)\n",
    "    train_start=time.time()\n",
    "    lr.fit(train_x, train_y)\n",
    "    train_finish=time.time()\n",
    "    train_time_seconds=train_finish-train_start\n",
    "\n",
    "    predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "    eval_metrics = u.evaluate_regression_metrics(test_y, predicted_qualities)\n",
    "    \n",
    "    # Logging numbers or objects saves both locally under the run folder sub directory and in the UI\n",
    "    \n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "    mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "    \n",
    "    # MLFlow can log any arbitrary user defined metric. For this purpose we will record\n",
    "    # training time and object size. This is useful to quickly see if an approach is not computationally\n",
    "    # feasible or to determine if any performance gains are worth the computational cost\n",
    "    model_memory_bytes=sys.getsizeof(lr)\n",
    "    mlflow.log_metric(\"train_time_seconds\",train_time_seconds)\n",
    "    mlflow.log_metric(\"model_memory_bytes\",model_memory_bytes)\n",
    "    \n",
    "    # MLFlow parameters and metrics can also be entered in as a dictionary\n",
    "    # See the singular VS plural version log_metric VS log_metrics, log_param VS log_params\n",
    "    mlflow.log_metrics(eval_metrics)\n",
    "\n",
    "    # MLFlow will log supplementary meta data (EX versioning, packages) alongside the model \n",
    "    mlflow.sklearn.log_model(lr, \"model\")\n",
    "    \n",
    "    # MLFlow can also log a a matplotlib figure\n",
    "    residual_fig=plot_residuals(valid_y=test_y,preds=predicted_qualities)\n",
    "    # model, metrics, params have dedicated sub folders\n",
    "    # other types of logged objects need a specified sub folder as such we save under plots\n",
    "    mlflow.log_figure(residual_fig,\"plots/elastic_net_example_residuals.png\")\n",
    "    \n",
    "    # MLflow can also log a plotly example. We will convert to plotly and log both side by side.\n",
    "    residual_fig_plotly=plotly.tools.mpl_to_plotly(residual_fig)\n",
    "    mlflow.log_figure(residual_fig_plotly, \"plots/elastic_net_example_residuals_plotly.html\")\n",
    "    \n",
    "    # MLFlow cannot directly log a data set. However it can upload most types of presaved file\n",
    "    # many of which can be viewed directly in the UI\n",
    "    # In this example we will calculate metrics by quality and save/log using various data types for comparison\n",
    "    eval_data:pd.DataFrame=test_x.\\\n",
    "        assign(quality=test_y,pred=predicted_qualities)\n",
    "    eval_data['error']=eval_data['quality'] - eval_data['pred']\n",
    "    eval_data['absolute_error']=np.abs(eval_data['error'])\n",
    "    eval_data['squared_error']=eval_data['error'].pow(2)\n",
    "    metrics_by_quality = eval_data.\\\n",
    "                groupby(\"quality\",as_index=False).\\\n",
    "                agg(mean_absolute_error=('absolute_error','mean'),\n",
    "                    mean_squared_error=('squared_error','mean')\n",
    "                    )\n",
    "                \n",
    "    extension_type_dataframe_save_method_name_dict = {\n",
    "        \"html\":'to_html',\n",
    "        \"csv\":'to_csv',\n",
    "        \"pkl\":'to_pickle',\n",
    "        \"md\":'to_markdown',\n",
    "        \"json\":'to_json'\n",
    "    }\n",
    "    for extension_name, method_name in extension_type_dataframe_save_method_name_dict.items():\n",
    "        save_func=getattr(metrics_by_quality, method_name)\n",
    "        save_file=os.path.join(current_file_path,\"../data_outputs/\",\"metrics_by_quality.\" + extension_name)\n",
    "        save_func(save_file)\n",
    "        # Unlike previous logging, the inputs are the file itself rather than a python object\n",
    "        mlflow.log_artifact(local_path=save_file,artifact_path=\"data\")\n",
    "        \n",
    "\n",
    "    mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
